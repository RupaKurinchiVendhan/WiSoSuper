{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was used to generate the input data for training and testing on the SOTA GANs model, and EDSR and ESRGAN. More information for accessing data from the WIND Toolkit and NSRDB can be found at the following resources:\r\n",
    "1. WIND Toolkit: https://www.nrel.gov/grid/wind-toolkit.html\r\n",
    "2. NSRDB: https://nsrdb.nrel.gov/\r\n",
    "3. Stand up your own HSDS server: https://github.com/HDFGroup/hsds\r\n",
    "4. Use the HDF groups Kita Lab (a managed HSDS service on AWS, for higher rate limits on free trial basis): https://www.hdfgroup.org/solutions/hdf-kita/\r\n",
    "5. HSDS Wind Examples: https://github.com/NREL/hsds-examples/blob/master/notebooks/01_WTK_introduction.ipynb\r\n",
    "6. HSDS Solar Examples: https://github.com/NREL/hsds-examples/blob/master/notebooks/03_NSRDB_introduction.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate TFRecords for input for the SOTA GANs method, use the method 'generate_TFRecords().' once you have data of the shape (N_batch, height, width, [ua, va]) or (N_batch, height, width, [DNI, DHI]), you can use the following line of code to generate your TFRecords:\n",
    "    \n",
    "    generate_TFRecords(\"filename\", data, mode='train', K=k)\n",
    "    \n",
    "This will output two TFRecords named \"filename_LR.tfrecord\" and \"filename_HR.tfrecord\", where the LR data represents the HR data, coarsened by a factor of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5pyd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' @authors: Andrew Glaws, Karen Stengel, Ryan King\n",
    "    modified by: Rupa Kurinchi-Vendhan\n",
    "'''\n",
    "tf.compat.v1.disable_eager_execution() # REMOVE THIS\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def downscale_image(x, K):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    if x.ndim == 3:\n",
    "        x = x.reshape((1, x.shape[0], x.shape[1], x.shape[2]))\n",
    "\n",
    "    x_in = tf.compat.v1.placeholder(tf.float64, [None, x.shape[1], x.shape[2], x.shape[3]])\n",
    "\n",
    "    weight = tf.constant(1.0/K**2, shape=[K, K, x.shape[3], x.shape[3]], dtype=tf.float64)\n",
    "    downscaled = tf.compat.v1.nn.conv2d(x_in, filter=weight, strides=[1, K, K, 1], padding='SAME')\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        ds_out = sess.run(downscaled, feed_dict={x_in: x})\n",
    "\n",
    "    return ds_out\n",
    "\n",
    "def generate_TFRecords(filename, data, mode='test', K=None):\n",
    "    '''\n",
    "    Generate TFRecords files for model training or testing.\n",
    "\n",
    "    Parameters:\n",
    "        filename - filename for TFRecord (should by type *.tfrecord)\n",
    "        data     - numpy array of size (N, h, w, c) containing data to be written to TFRecord\n",
    "        model    - if 'train', then data contains HR data that is coarsened k times\n",
    "                   and both HR and LR data written to TFRecord\n",
    "                   if 'test', then data contains LR data\n",
    "        K        - downscaling factor, must be specified in training mode\n",
    "    '''\n",
    "    if mode == 'train':\n",
    "        assert K is not None, 'In training mode, downscaling factor K must be specified'\n",
    "        data_LR = downscale_image(data, K)\n",
    "        data_HR = downscale_image(data, 1)\n",
    "\n",
    "    LR_filename = filename + \"_LR.tfrecord\"\n",
    "    HR_filename = filename + \"_HR.tfrecord\"\n",
    "\n",
    "    with tf.compat.v1.python_io.TFRecordWriter(LR_filename) as writer:\n",
    "        for j in range(data.shape[0]):\n",
    "            h_LR, w_LR, c = data_LR[j, ...].shape\n",
    "            features = tf.train.Features(feature={\n",
    "                                    'index': _int64_feature(j),\n",
    "                                'data_LR': _bytes_feature(data_LR[j, ...].tostring()),\n",
    "                                    'h_LR': _int64_feature(h_LR),\n",
    "                                    'w_LR': _int64_feature(w_LR),\n",
    "                                        'c': _int64_feature(c)})\n",
    "            example = tf.train.Example(features=features)\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "    with tf.compat.v1.python_io.TFRecordWriter(HR_filename) as writer:\n",
    "        for j in range(data.shape[0]):\n",
    "            h_LR, w_LR, c = data_HR[j, ...].shape\n",
    "            features = tf.train.Features(feature={\n",
    "                                    'index': _int64_feature(j),\n",
    "                                'data_LR': _bytes_feature(data_HR[j, ...].tostring()),\n",
    "                                    'h_LR': _int64_feature(h_LR),\n",
    "                                    'w_LR': _int64_feature(w_LR),\n",
    "                                        'c': _int64_feature(c)})\n",
    "            example = tf.train.Example(features=features)\n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input HR data had dimensions 100 $\\times$ 100, which was coarsened using bicubic interpolation to have dimensions 25 $\\times$ 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_change_scale(img, dimension, scale=100, interpolation=cv2.INTER_CUBIC):\n",
    "    '''\n",
    "    Resize image to a specificall scale of original image.\n",
    "    \n",
    "    Parameters:\n",
    "        img       - original image (should be type numpy.ndarray)\n",
    "        dimension - original image dimension (should be type tuple)\n",
    "        scale     - integer scale factor to multiply the size of the original image\n",
    "\n",
    "    Returns:\n",
    "        resized_image - resized image (type numpy.ndarray)\n",
    "    '''\n",
    "    scale /= 100\n",
    "    new_dimension = (int(dimension[1]*scale), int(dimension[0]*scale))\n",
    "    resized_img = cv2.resize(img, new_dimension, interpolation=interpolation)\n",
    "\n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wind Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wind velocity data is comprised of northerly and easterly wind components, denoted $v$ and $u$ respectively, calculated from 100-m height wind speed and direction. The WIND Toolkit has a spatial resolution of approximately 2-km $\\times$ 2-km. The training data was randomly sampled at a 4-hourly temporal resolution for the years 2007 to 2013, starting January 1, 2007 12 am. Wind test data was randomly sampled at a 4-hourly temporal resolution for the same time span, starting January 1, 2007 2 am."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import h5pyd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5pyd.File(\"/nrel/wtk-us.h5\", 'r', bucket=\"nrel-pds-hsds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_timesteps = random.sample(range(0, 61368, 4), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_speed = f['windspeed_100m']\n",
    "dset_dir = f['winddirection_100m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestep in wind_timesteps:\n",
    "    speed_HR = dset_speed[timestep,::,::]\n",
    "    direction_HR = dset_dir[timestep,::,::]\n",
    "    speed_HR = speed_HR[:1600,500:2100]\n",
    "    direction_HR = direction_HR[:1600,500:2100]\n",
    "    ua_HR = np.multiply(speed_HR, np.cos(np.radians(direction_HR+np.pi/2)))\n",
    "    va_HR = np.multiply(speed_HR, np.sin(np.radians(direction_HR+np.pi/2)))\n",
    "    \n",
    "    h_HR = 100\n",
    "    w_HR = 100\n",
    "    h_LR = 25\n",
    "    w_LR = 25\n",
    "    \n",
    "    ua_LR = image_change_scale(ua_HR, (h_HR, w_HR), 25)\n",
    "    va_LR = image_change_scale(va_HR, (h_HR, w_HR), 25)\n",
    "    \n",
    "    ua_wind_data_HR = np.zeros(shape=(256, 100, 100))\n",
    "    ua_wind_data_LR = np.zeros(shape=(256, 25, 25))\n",
    "    va_wind_data_HR = np.zeros(shape=(256, 100, 100))\n",
    "    va_wind_data_LR = np.zeros(shape=(256, 25, 25))\n",
    "    wind_data = np.zeros((256, 100, 100, 2))\n",
    "    \n",
    "    idx = 0\n",
    "    for row in range(16):\n",
    "        for col in range(16):\n",
    "            ua_wind_data_HR[idx] = ua_HR[(col*h_HR):(h_HR+col*h_HR), (row*w_HR):(w_HR+row*w_HR)]\n",
    "            ua_wind_data_LR[idx] = ua_LR[(col*h_LR):(h_LR+col*h_LR), (row*w_LR):(w_LR+row*w_LR)]\n",
    "            va_wind_data_HR[idx] = va_HR[(col*h_HR):(h_HR+col*h_HR), (row*w_HR):(w_HR+row*w_HR)]\n",
    "            va_wind_data_LR[idx] = va_LR[(col*h_LR):(h_LR+col*h_LR), (row*w_LR):(w_LR+row*w_LR)]\n",
    "            wind_data[idx] = np.dstack([ua_wind_data_HR[idx],va_wind_data_HR[idx]])\n",
    "            \n",
    "            ua_filename = \"ua_{timestep}_{idx}.png\".format(timestep=timestep, idx=idx)\n",
    "            va_filename = \"va_{timestep}_{idx}.png\".format(timestep=timestep, idx=idx)\n",
    "            \n",
    "            plt.imsave(\"train/edsr/wind/LR/\"+ua_filename, ua_wind_data_LR[idx], origin='lower', format=\"png\")\n",
    "            plt.imsave(\"train/edsr/wind/HR/\"+ua_filename, ua_wind_data_HR[idx], origin='lower', format=\"png\")\n",
    "            plt.imsave(\"train/edsr/wind/LR/\"+va_filename, va_wind_data_LR[idx], origin='lower', format=\"png\")\n",
    "            plt.imsave(\"train/edsr/wind/HR/\"+va_filename, va_wind_data_HR[idx], origin='lower', format=\"png\")\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_wind_timesteps = random.sample(range(2, 61368, 4), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestep in test_wind_timesteps:\n",
    "    speed_HR = dset_speed[timestep,::,::]\n",
    "    direction_HR = dset_dir[timestep,::,::]\n",
    "    speed_HR = speed_HR[:1600,500:2100]\n",
    "    direction_HR = direction_HR[:1600,500:2100]\n",
    "    ua_HR = np.multiply(speed_HR, np.cos(np.radians(direction_HR+np.pi/2)))\n",
    "    va_HR = np.multiply(speed_HR, np.sin(np.radians(direction_HR+np.pi/2)))\n",
    "    \n",
    "    h_HR = 100\n",
    "    w_HR = 100\n",
    "    h_LR = 25\n",
    "    w_LR = 25\n",
    "    \n",
    "    ua_LR = image_change_scale(ua_HR, (h_HR, w_HR), 25)\n",
    "    va_LR = image_change_scale(va_HR, (h_HR, w_HR), 25)\n",
    "    \n",
    "    ua_wind_data_HR = np.zeros(shape=(256, 100, 100))\n",
    "    ua_wind_data_LR = np.zeros(shape=(256, 25, 25))\n",
    "    va_wind_data_HR = np.zeros(shape=(256, 100, 100))\n",
    "    va_wind_data_LR = np.zeros(shape=(256, 25, 25))\n",
    "    wind_data = np.zeros((256, 100, 100, 2))\n",
    "    \n",
    "    idx = 0\n",
    "    for row in range(16):\n",
    "        for col in range(16):\n",
    "            ua_wind_data_HR[idx] = ua_HR[(col*h_HR):(h_HR+col*h_HR), (row*w_HR):(w_HR+row*w_HR)]\n",
    "            ua_wind_data_LR[idx] = ua_LR[(col*h_LR):(h_LR+col*h_LR), (row*w_LR):(w_LR+row*w_LR)]\n",
    "            va_wind_data_HR[idx] = va_HR[(col*h_HR):(h_HR+col*h_HR), (row*w_HR):(w_HR+row*w_HR)]\n",
    "            va_wind_data_LR[idx] = va_LR[(col*h_LR):(h_LR+col*h_LR), (row*w_LR):(w_LR+row*w_LR)]\n",
    "            wind_data[idx] = np.dstack([ua_wind_data_HR[idx],va_wind_data_HR[idx]])\n",
    "            \n",
    "            ua_filename = \"ua_{timestep}_{idx}.png\".format(timestep=timestep, idx=idx)\n",
    "            va_filename = \"va_{timestep}_{idx}.png\".format(timestep=timestep, idx=idx)\n",
    "            \n",
    "            plt.imsave(\"test/edsr/wind/LR/\"+ua_filename, ua_wind_data_LR[idx], origin='lower', format=\"png\")\n",
    "            plt.imsave(\"test/edsr/wind/HR/\"+ua_filename, ua_wind_data_HR[idx], origin='lower', format=\"png\")\n",
    "            plt.imsave(\"test/edsr/wind/LR/\"+va_filename, va_wind_data_LR[idx], origin='lower', format=\"png\")\n",
    "            plt.imsave(\"test/edsr/wind/HR/\"+va_filename, va_wind_data_HR[idx], origin='lower', format=\"png\")\n",
    "            idx += 1\n",
    "    generate_TFRecords(\"test/gans/wind/wind_{timestep}\".format(timestep=timestep), wind_data, mode='test', K=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solar Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider solar irradiance data in terms of direct normal irradiance (DNI) and diffused horizontal irradiance (DHI) at a 4-km spatial resolution. The training data was obtained by randomly sampling data at an hourly temporal resolution from  6 am to 6 pm for the years 2007 to 2013. Solar test data was randomly sampled at an hourly temporal resolution from 6 am to 6 pm for the years 2014 to 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5pyd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NSRDB offers data from 1998 to 2020. To keep consistent with the WIND Toolkit, we used test data from 2007-2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrel_admin                                          folder   2020-05-31 19:52:14 /nrel/nsrdb/v3/\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:02:30 /nrel/nsrdb/v3/nsrdb_1998.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:03:08 /nrel/nsrdb/v3/nsrdb_1999.h5\n",
      "nrel_admin                              9.8M        domain   2020-07-13 21:22:01 /nrel/nsrdb/v3/nsrdb_2000.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:03:00 /nrel/nsrdb/v3/nsrdb_2001.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:03:10 /nrel/nsrdb/v3/nsrdb_2002.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:02:30 /nrel/nsrdb/v3/nsrdb_2003.h5\n",
      "nrel_admin                              9.8M        domain   2020-07-06 22:18:20 /nrel/nsrdb/v3/nsrdb_2004.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:02:48 /nrel/nsrdb/v3/nsrdb_2005.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:02:45 /nrel/nsrdb/v3/nsrdb_2006.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:03:04 /nrel/nsrdb/v3/nsrdb_2007.h5\n",
      "nrel_admin                              9.8M        domain   2020-07-13 21:21:31 /nrel/nsrdb/v3/nsrdb_2008.h5\n",
      "nrel_admin                              9.8M        domain   2020-07-13 21:22:08 /nrel/nsrdb/v3/nsrdb_2009.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:03:35 /nrel/nsrdb/v3/nsrdb_2010.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:03:16 /nrel/nsrdb/v3/nsrdb_2011.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:02:48 /nrel/nsrdb/v3/nsrdb_2012.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:03:10 /nrel/nsrdb/v3/nsrdb_2013.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:03:45 /nrel/nsrdb/v3/nsrdb_2014.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:02:48 /nrel/nsrdb/v3/nsrdb_2015.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:02:52 /nrel/nsrdb/v3/nsrdb_2016.h5\n",
      "nrel_admin                              9.8M        domain   2020-06-05 22:02:53 /nrel/nsrdb/v3/nsrdb_2017.h5\n",
      "nrel_admin                              9.4M        domain   2020-06-18 20:55:10 /nrel/nsrdb/v3/nsrdb_2018.h5\n",
      "nrel_admin                              1.5T        domain   2020-07-17 15:59:04 /nrel/nsrdb/v3/nsrdb_2019.h5\n",
      "nrel_admin                              1.5T        domain   2021-06-29 21:05:56 /nrel/nsrdb/v3/nsrdb_2020.h5\n",
      "nrel_admin                                          folder   2020-10-16 21:59:06 /nrel/nsrdb/v3/tdy\n",
      "nrel_admin                                          folder   2020-10-16 21:59:11 /nrel/nsrdb/v3/tgy\n",
      "nrel_admin                                          folder   2020-10-16 21:59:00 /nrel/nsrdb/v3/tmy\n",
      "27 items\n"
     ]
    }
   ],
   "source": [
    "! hsls -H -v --bucket nrel-pds-hsds /nrel/nsrdb/v3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_solar_data(f, timestep):\n",
    "    '''\n",
    "    Formats solar data to take the form (height, width, [DNI, DHI]).\n",
    "    \n",
    "    Parameters:\n",
    "       f        - HDF5 file \"nsrdb_{year}.h5\" (mode r)\n",
    "       timestep - NSRDB time index during the given year, ranging from 0 to 17520\n",
    "    \n",
    "    Returns:\n",
    "        solar_2d - solar data of the shape (height, width, [DNI, DHI]) (type numpy.ndarray)\n",
    "    '''\n",
    "    dni = f['dni']\n",
    "    dhi = f['dhi']\n",
    "\n",
    "    coords = f['coordinates'][...]\n",
    "    lons = coords[::10, 1]\n",
    "    lats = coords[::10, 0]\n",
    "\n",
    "    lats_unique = np.unique(lats)\n",
    "    lons_unique = np.unique(lons)\n",
    "\n",
    "    coords_2d = np.zeros((lats.shape[0], lons.shape[0], 2))\n",
    "    \n",
    "    for lat_i, lat in enumerate(lats_unique):\n",
    "        for lon_i, lon in enumerate(lons_unique):\n",
    "            coords_2d[lat_i, lon_i, 0] = lat\n",
    "            coords_2d[lat_i, lon_i, 1] = lon\n",
    "\n",
    "    solar_2d = np.zeros(coords_2d.shape)\n",
    "\n",
    "    for c, coord in enumerate(coords):\n",
    "        lat = coord[0]\n",
    "        lon = coord[1]\n",
    "\n",
    "        lat_index = np.where(lats_unique == lat)[0]\n",
    "        lon_index = np.where(lons_unique == lon)[0]\n",
    "\n",
    "        solar_2d[lat_index, lon_index, 0] = dni[timestep, c]\n",
    "        solar_2d[lat_index, lon_index, 1] = dhi[timestep, c]\n",
    "        \n",
    "    return solar_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this for each year 2007-2013 and add to a list of 400 timesteps over the entire timespan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5pyd.File(\"/nrel/nsrdb/v3/nsrdb_2007.h5\", 'r', bucket=\"nrel-pds-hsds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_timesteps = random.sample(range(24, 17520, 48), 57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestep in solar_timesteps:\n",
    "    solar_data = format_solar_data(f, timestep)[:1600, :1600, ::]\n",
    "    dni_HR = solar_data[::, ::, 0]\n",
    "    dhi_HR = solar_data[::, ::, 1]\n",
    "    \n",
    "    h_HR = 100\n",
    "    w_HR = 100\n",
    "    h_LR = 25\n",
    "    w_LR = 25\n",
    "    \n",
    "    dni_LR = image_change_scale(dni_HR, (h_HR, w_HR), 25)\n",
    "    dhi_LR = image_change_scale(dhi_HR, (h_HR, w_HR), 25)\n",
    "    \n",
    "    dni_solar_data_HR = np.zeros(shape=(256, 100, 100))\n",
    "    dni_solar_data_LR = np.zeros(shape=(256, 25, 25))\n",
    "    dhi_solar_data_HR = np.zeros(shape=(256, 100, 100))\n",
    "    dhi_solar_data_LR = np.zeros(shape=(256, 25, 25))\n",
    "\n",
    "    idx=0\n",
    "    for row in range(10):\n",
    "        for col in range(10):\n",
    "            dni_solar_data_HR[idx] = dni_HR[(col*h_HR):(h_HR+col*h_HR), (row*w_HR):(w_HR+row*w_HR)]\n",
    "            dni_solar_data_LR[idx] = dni_LR[(col*h_LR):(h_LR+col*h_LR), (row*w_LR):(w_LR+row*w_LR)]\n",
    "            dhi_solar_data_HR[idx] = dhi_HR[(col*h_HR):(h_HR+col*h_HR), (row*w_HR):(w_HR+row*w_HR)]\n",
    "            dhi_solar_data_LR[idx] = dhi_LR[(col*h_LR):(h_LR+col*h_LR), (row*w_LR):(w_LR+row*w_LR)]\n",
    "            \n",
    "            dni_filename = \"dni_{timestep}_{idx}\".format(timestep=timestep, idx=idx)\n",
    "            dhi_filename = \"dhi_{timestep}_{idx}\".format(timestep=timestep, idx=idx)\n",
    "            \n",
    "            plt.imsave(\"train/edsr/solar/LR/\"+dni_filename+\".png\", dni_solar_data_LR[idx], cmap=\"inferno\", format=\"png\")\n",
    "            plt.imsave(\"train/edsr/solar/HR/\"+dni_filename+\".png\", dni_solar_data_HR[idx], cmap=\"inferno\", format=\"png\")\n",
    "            plt.imsave(\"train/edsr/solar/LR/\"+dhi_filename+\".png\", dhi_solar_data_LR[idx], cmap=\"inferno\", format=\"png\")\n",
    "            plt.imsave(\"train/edsr/solar/HR/\"+dhi_filename+\".png\", dhi_solar_data_HR[idx], cmap=\"inferno\", format=\"png\")\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this for each year 2014-2018 and add to a list of 800 timesteps over the entire timespan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5pyd.File(\"/nrel/nsrdb/v3/nsrdb_2014.h5\", 'r', bucket=\"nrel-pds-hsds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solar_timesteps = random.sample(range(24, 17520, 48), 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestep in test_solar_timesteps:\n",
    "    solar_data = format_solar_data(f, timestep)[:1600, 1300:2900, ::]\n",
    "    dni_HR = solar_data[::, ::, 0]\n",
    "    dhi_HR = solar_data[::, ::, 1]\n",
    "    \n",
    "    dni_LR = image_change_scale(dni_HR, (h_HR, w_HR), 25)\n",
    "    dhi_LR = image_change_scale(dhi_HR, (h_HR, w_HR), 25)\n",
    "    \n",
    "    dni_solar_data_HR = np.zeros(shape=(256, 100, 100))\n",
    "    dni_solar_data_LR = np.zeros(shape=(256, 25, 25))\n",
    "    dhi_solar_data_HR = np.zeros(shape=(256, 100, 100))\n",
    "    dhi_solar_data_LR = np.zeros(shape=(256, 25, 25))\n",
    "    solar_data = np.zeros((256, 100, 100, 2))\n",
    "    \n",
    "    h_HR = 100\n",
    "    w_HR = 100\n",
    "    \n",
    "    h_LR = 25\n",
    "    w_LR = 25\n",
    "    idx=0\n",
    "    for row in range(10):\n",
    "        for col in range(10):\n",
    "            dni_solar_data_HR[idx] = dni_HR[(col*h_HR):(h_HR+col*h_HR), (row*w_HR):(w_HR+row*w_HR)]\n",
    "            dni_solar_data_LR[idx] = dni_LR[(col*h_LR):(h_LR+col*h_LR), (row*w_LR):(w_LR+row*w_LR)]\n",
    "            dhi_solar_data_HR[idx] = dhi_HR[(col*h_HR):(h_HR+col*h_HR), (row*w_HR):(w_HR+row*w_HR)]\n",
    "            dhi_solar_data_LR[idx] = dhi_LR[(col*h_LR):(h_LR+col*h_LR), (row*w_LR):(w_LR+row*w_LR)]\n",
    "            solar_data[idx] = np.dstack([dni_solar_data_HR[idx],dhi_solar_data_HR[idx]])\n",
    "            \n",
    "            dni_filename = \"dni_{timestep}_{idx}\".format(timestep=timestep, idx=idx)\n",
    "            dhi_filename = \"dhi_{timestep}_{idx}\".format(timestep=timestep, idx=idx)\n",
    "            \n",
    "            plt.imsave(\"test/edsr/solar/LR/\"+dni_filename+\".png\", dni_solar_data_LR[idx], cmap=\"inferno\", format=\"png\")\n",
    "            plt.imsave(\"test/edsr/solar/HR/\"+dni_filename+\".png\", dni_solar_data_HR[idx], cmap=\"inferno\", format=\"png\")\n",
    "            plt.imsave(\"test/edsr/solar/LR/\"+dhi_filename+\".png\", dhi_solar_data_LR[idx], cmap=\"inferno\", format=\"png\")\n",
    "            plt.imsave(\"test/edsr/solar/HR/\"+dhi_filename+\".png\", dhi_solar_data_HR[idx], cmap=\"inferno\", format=\"png\")\n",
    "            idx += 1\n",
    "    generate_TFRecords(\"test/gans/solar tfrecords/solar_{timestep}\".format(timestep=timestep), solar_data, mode='train', K=5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d6e87419faf0c4d576628b629beb80feca08f3f6f53562b53100002f0541799"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}